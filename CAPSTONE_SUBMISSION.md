# Capstone Submission 

This document summarizes the deliverables, reproducibility instructions, evaluation against a typical capstone rubric, known limitations, and suggested next steps. Include this file (or its contents) when you submit the project.

## Deliverables Included

- Source code: all Python modules (`app_streamlit.py`, `simulate.py`, `signal_model.py`, `backtest.py`, `ml_models.py`, `run_ml_baseline.py`, `sequence_modeling.py`).
- Tests: `tests/test_signal_model.py`, `tests/test_backtest.py` (run with `pytest`).
- Interactive app: `app_streamlit.py` (Streamlit UI with Real Data, Explore, Calibrate, Backtest, ML, Drilldown tabs).
- Artifacts: `artifacts/ml_comparison.json`, `artifacts/feature_importance_rf.png`, `artifacts/feature_importance_xgb.png` (generated by `run_ml_baseline.py`).
- Documentation: `README.md` and this `CAPSTONE_SUBMISSION.md`.

## How to Reproduce (recommended)

1. Clone repository and create virtual environment:

```powershell
git clone <repo-url>
cd "D:\Credit Card"
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

2. (Optional) Install extra ML libs for XGBoost/LightGBM:

```powershell
pip install xgboost lightgbm
```

3. Run tests:

```powershell
python -m pytest -q
```

4. Run Streamlit app:

```powershell
python -m streamlit run app_streamlit.py
```

5. Run ML baseline to regenerate artifacts:

```powershell
python run_ml_baseline.py
```

6. Review `artifacts/` for `ml_comparison.json` and feature importance images.

## Rubric Mapping 

- Problem Definition & Impact: Clear risk detection goal and business impact examples; includes proposed pilot & scale plan.
- Data & Feature Engineering: Signal engineering implemented in `signal_model.py` (grouped behavioral features). Documentation explains the signals, but real-data feature engineering needs more detail for production.
- Modeling: Provides rule-based scoring and ML baselines (RandomForest, optional XGBoost). Includes evaluation (PR-AUC, ROC-AUC, precision/recall at thresholds).
- Validation & Backtesting: `backtest.py` injects outcomes and computes validation metrics; `run_ml_baseline.py` produces comparative artifacts.
- Engineering & Reproducibility: venv instructions, `requirements.txt`, CI workflow, Dockerfile, and in-app ML tab for reproducible runs.
- Presentation: Streamlit app with drilldown, PDF export, and webhook stub for alerting.

## Limitations 

- Heavy reliance on synthetic data for validation; performance on real labeled data may differ substantially.
- Simplified outcome injection in `backtest.py` â€” correlation model is linear and synthetic.
- Limited feature engineering for transactional raw data; no advanced time-series feature extraction pipeline included.
- No deployed production pipeline (this is a prototype); integrations (CRM, SMS providers) need engineering.

## Next Steps (Future enhancements)

1. Run and validate on a labeled real dataset (12 months) and produce final ROC/PR curves.
2. Add cross-validation and hyperparameter tuning (Optuna / sklearn CV) for tree models.
3. Implement rolling-window feature pipelines and store preprocessed datasets to `data/processed/`.
4. Prototype sequence models with `sequence_modeling.py` and small TF experiments if temporal signals are significant.
5. Add cost-sensitive evaluation (FP vs FN cost matrix) and simple A/B design to measure intervention effectiveness.

---
